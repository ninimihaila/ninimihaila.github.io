<!doctype html>
<html>
<head>
  <title>Emergent strong AI</title>
  <style>
  #content {
  	margin-top: 50pt;
  	margin-left: 25%;
    width: 50%;
    font-family: Georgia, sans-serif;
  }
  </style>
</head>
<body>
  <div id="content">
<p>Take the chinese room thought experiment.</p>
<p>If we create a program that behaves as if it understand Chinese; it takes Chinese characters as input, and by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose it does this task so well that it passes the Turing test. </p>
<p>The question is: does the machine literally "understand" Chinese, or is it merely simulating the ability to understand Chinese? (strong AI vs weak AI)</p>
<p>If a person is in a closed room and has a book with an English version of the computer program, along with sufficient paper, pencils, erasers etc., then he could receive Chinese characters through a door, process them according to the program's instructions, and produce Chinese characters as output. </p>
<p>If the computer passed the Turing test this way, then he would do so as well, by running the program manually.</p>
<p>Both he and the computer follow a program, step-by-step, producing a behaviour which is interpreted as demonstrating intelligent conversation. However, the person would not be able to understand the conversation. Therefore, the computer would not be able to understand the conversation either.</p>
<p>Without understanding, we cannot describe what the machine is doing as "thinking", and since it does not think, it does not have a "mind" (strong AI is impossible)</p>
<p></p>
<p>The chinese room is a Turing complete machine (it has a program, memory - the papers and cabinets, a CPU - the man that follows the instructions, and a means to write to memory - the pencil and eraser). Therefore it is capable of doing a step-by-step simulation of any other Turing machine (given enough memory and time). Thus, if the Chinese room does not or can not contain a Chinese speaking mind, then no other digital computer can contain a mind.</p>
<p></p>
<p>This argument basically states that a computer running a fixed (classical) program cannot have a mind/conciousness/understanding of what it's doing because it's behaviour can be imitated by a human running the program manually.</p>
<p>The question is, does this still apply if the program is more complex, i.e. a machine learning/AI algorithm that simulates neurons, and the behaviour is emergent from their interactions? </p>
<p>Aren't we actually just complex machines whose behaviour emerges from the interactions of our neurons? The neurons themselves don't have any understanding of what all of them are doing, but the emergent consciense is the result of all of them acting together. The appearence of consciousness and actual consciousness are one and the same.</p>
  </div>
</body>
</html>
